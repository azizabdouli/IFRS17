#!/usr/bin/env python3
"""
Test rapide des modÃ¨les ML aprÃ¨s correction des index
"""

import sys
import os
import pandas as pd
from pathlib import Path

# Ajouter le rÃ©pertoire parent au PATH
sys.path.append(str(Path(__file__).parent))

def test_models_after_fix():
    """Test rapide des modÃ¨les aprÃ¨s correction"""
    print("ğŸ”§ Test des modÃ¨les ML aprÃ¨s correction des index...")
    
    try:
        from backend.ml.ml_service import MLService
        
        ml_service = MLService()
        print("âœ… MLService initialisÃ©")
        
        # Charger Ã©chantillon de donnÃ©es
        data_path = "Data/Ppna (4).xlsx"
        df = pd.read_excel(data_path)
        sample_df = df.sample(n=500, random_state=42)  # Plus petit Ã©chantillon
        print(f"ğŸ“Š Test avec Ã©chantillon: {len(sample_df)} lignes")
        
        results = {}
        
        # Test 1: Profitability
        print("\nğŸ’° Test modÃ¨le de rentabilitÃ©...")
        try:
            result = ml_service.train_profitability_model(sample_df)
            r2 = result.get('r2_score', 0)
            results['profitability'] = f"âœ… RÂ² = {r2:.3f}"
            print(f"âœ… Profitability: RÂ² = {r2:.3f}")
        except Exception as e:
            results['profitability'] = f"âŒ {str(e)[:100]}..."
            print(f"âŒ Profitability error: {e}")
        
        # Test 2: Risk Classification
        print("\nâš ï¸ Test classification des risques...")
        try:
            result = ml_service.train_risk_classification_model(sample_df)
            acc = result.get('accuracy', 0)
            results['risk_classification'] = f"âœ… Accuracy = {acc:.3f}"
            print(f"âœ… Risk classification: Accuracy = {acc:.3f}")
        except Exception as e:
            results['risk_classification'] = f"âŒ {str(e)[:100]}..."
            print(f"âŒ Risk classification error: {e}")
        
        # Test 3: Claims Prediction
        print("\nğŸ” Test prÃ©diction des sinistres...")
        try:
            result = ml_service.train_claims_prediction_model(sample_df)
            r2 = result.get('r2_score', 0)
            results['claims_prediction'] = f"âœ… RÂ² = {r2:.3f}"
            print(f"âœ… Claims prediction: RÂ² = {r2:.3f}")
        except Exception as e:
            results['claims_prediction'] = f"âŒ {str(e)[:100]}..."
            print(f"âŒ Claims prediction error: {e}")
        
        # Test 4: LRC Prediction
        print("\nğŸ“ˆ Test prÃ©diction LRC...")
        try:
            result = ml_service.train_lrc_prediction_model(sample_df)
            r2 = result.get('r2_score', 0)
            results['lrc_prediction'] = f"âœ… RÂ² = {r2:.3f}"
            print(f"âœ… LRC prediction: RÂ² = {r2:.3f}")
        except Exception as e:
            results['lrc_prediction'] = f"âŒ {str(e)[:100]}..."
            print(f"âŒ LRC prediction error: {e}")
        
        # RÃ©sumÃ©
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SULTATS DU TEST POST-CORRECTION")
        print("="*60)
        
        success_count = 0
        for model_name, result in results.items():
            print(f"{result}")
            if "âœ…" in result:
                success_count += 1
        
        print(f"\nğŸ¯ ModÃ¨les fonctionnels: {success_count}/{len(results)}")
        
        if success_count == len(results):
            print("\nğŸ‰ TOUTES LES CORRECTIONS RÃ‰USSIES !")
            print("ğŸš€ SystÃ¨me ML IFRS17 totalement opÃ©rationnel")
        else:
            print(f"\nâš ï¸ {len(results) - success_count} modÃ¨le(s) nÃ©cessitent encore attention")
        
        return success_count == len(results)
        
    except Exception as e:
        print(f"âŒ Erreur critique: {e}")
        return False

if __name__ == "__main__":
    test_models_after_fix()
# ml/optimized_ml_service.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
import os
from datetime import datetime
from functools import lru_cache
import asyncio
import concurrent.futures
from cachetools import TTLCache
import hashlib

from .data_preprocessing import OptimizedDataPreprocessor
from .ml_service import OptimizedMLService as BaseMLService  # Service de base
from .models.insurance_models import (
    ContractClusteringModel,
    ClaimsPredictionModel,
    ProfitabilityModel,
    RiskClassificationModel,
    AnomalyDetectionModel,
    LRCPredictionModel
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedMLService(BaseMLService):
    """
    Service ML optimisÃ© avec cache, lazy loading et traitement asynchrone
    HÃ©rite du service original pour compatibilitÃ©
    """
    
    def __init__(self, max_workers: int = 4, cache_ttl: int = 3600):
        # Initialiser le service parent
        super().__init__()
        
        # Remplacer le preprocessor par la version optimisÃ©e
        self.preprocessor = OptimizedDataPreprocessor()
        
        # Optimisations
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.model_cache = TTLCache(maxsize=64, ttl=cache_ttl)
        self.data_cache = TTLCache(maxsize=32, ttl=cache_ttl)
        
        logger.info("ðŸš€ Service ML optimisÃ© initialisÃ©")
    
    def _get_data_hash(self, df: pd.DataFrame) -> str:
        """GÃ©nÃ¨re un hash pour les donnÃ©es"""
        sample_data = df.head(100) if len(df) > 100 else df
        return hashlib.md5(pd.util.hash_pandas_object(sample_data).values).hexdigest()
    
    @lru_cache(maxsize=16)
    def _get_file_info(self, data_path: str) -> dict:
        """Cache des infos de fichier"""
        return {
            'size': os.path.getsize(data_path),
            'modified': os.path.getmtime(data_path),
            'extension': os.path.splitext(data_path)[1]
        }
    
    def load_and_preprocess_data(self, data_path: str) -> pd.DataFrame:
        """
        Chargement optimisÃ© des donnÃ©es avec cache
        """
        file_info = self._get_file_info(data_path)
        cache_key = f"{data_path}_{file_info['modified']}"
        
        # VÃ©rifier le cache
        if cache_key in self.data_cache:
            logger.info("ðŸ“ˆ DonnÃ©es rÃ©cupÃ©rÃ©es du cache")
            return self.data_cache[cache_key]
        
        logger.info(f"ðŸ“ Chargement optimisÃ© ({file_info['size']/1024/1024:.1f}MB)")
        
        # Chargement optimisÃ© selon la taille
        if file_info['size'] > 50 * 1024 * 1024:  # > 50MB
            df = self._load_large_file(data_path, file_info['extension'])
        else:
            df = self._load_small_file(data_path, file_info['extension'])
        
        logger.info(f"âœ… DonnÃ©es chargÃ©es: {df.shape[0]:,} lignes, {df.shape[1]} colonnes")
        
        # Cache le rÃ©sultat
        self.data_cache[cache_key] = df
        return df
    
    def _load_small_file(self, data_path: str, extension: str) -> pd.DataFrame:
        """Chargement des petits fichiers"""
        if extension == '.xlsx':
            # Optimisations pour Excel
            return pd.read_excel(data_path, engine='openpyxl')
        elif extension == '.csv':
            # Optimisations pour CSV
            return pd.read_csv(data_path, low_memory=False)
        else:
            raise ValueError("Format non supportÃ©. Utilisez .xlsx ou .csv")
    
    def _load_large_file(self, data_path: str, extension: str) -> pd.DataFrame:
        """Chargement optimisÃ© des gros fichiers"""
        logger.info("ðŸ“Š Fichier volumineux - chargement par chunks")
        
        if extension == '.csv':
            # Chargement par chunks pour CSV
            chunks = []
            chunk_size = 10000
            
            for chunk in pd.read_csv(data_path, chunksize=chunk_size, low_memory=False):
                chunks.append(chunk)
                
            return pd.concat(chunks, ignore_index=True)
        else:
            # Pour Excel, chargement normal (chunks non supportÃ©s)
            return self._load_small_file(data_path, extension)
    
    def train_claims_prediction_model(self, df: pd.DataFrame, target_column: str = None, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Version optimisÃ©e avec cache
        """
        # GÃ©nÃ©rer clÃ© de cache
        data_hash = self._get_data_hash(df)
        cache_key = f"claims_{model_type}_{data_hash}"
        
        # VÃ©rifier le cache
        if cache_key in self.model_cache:
            logger.info("ðŸ“ˆ ModÃ¨le de sinistres rÃ©cupÃ©rÃ© du cache")
            return self.model_cache[cache_key]
        
        # Utiliser la mÃ©thode parent si pas en cache
        logger.info("ðŸŽ¯ EntraÃ®nement optimisÃ© du modÃ¨le de sinistres")
        result = super().train_claims_prediction_model(df, target_column, model_type)
        
        # Mettre en cache
        self.model_cache[cache_key] = result
        return result
    
    def train_profitability_model(self, df: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Version optimisÃ©e avec cache
        """
        data_hash = self._get_data_hash(df)
        cache_key = f"profitability_{model_type}_{data_hash}"
        
        if cache_key in self.model_cache:
            logger.info("ðŸ“ˆ ModÃ¨le de rentabilitÃ© rÃ©cupÃ©rÃ© du cache")
            return self.model_cache[cache_key]
        
        logger.info("ðŸ’° EntraÃ®nement optimisÃ© du modÃ¨le de rentabilitÃ©")
        result = super().train_profitability_model(df, model_type)
        
        self.model_cache[cache_key] = result
        return result
    
    def train_risk_classification_model(self, df: pd.DataFrame, model_type: str = 'random_forest') -> Dict[str, Any]:
        """
        Version optimisÃ©e avec cache
        """
        data_hash = self._get_data_hash(df)
        cache_key = f"risk_{model_type}_{data_hash}"
        
        if cache_key in self.model_cache:
            logger.info("ðŸ“ˆ ModÃ¨le de classification rÃ©cupÃ©rÃ© du cache")
            return self.model_cache[cache_key]
        
        logger.info("âš ï¸ EntraÃ®nement optimisÃ© du modÃ¨le de classification")
        result = super().train_risk_classification_model(df, model_type)
        
        self.model_cache[cache_key] = result
        return result
    
    def perform_contract_clustering(self, df: pd.DataFrame, n_clusters: int = 5, clustering_type: str = 'kmeans') -> Dict[str, Any]:
        """
        Version optimisÃ©e avec cache
        """
        data_hash = self._get_data_hash(df)
        cache_key = f"clustering_{clustering_type}_{n_clusters}_{data_hash}"
        
        if cache_key in self.model_cache:
            logger.info("ðŸ“ˆ Clustering rÃ©cupÃ©rÃ© du cache")
            return self.model_cache[cache_key]
        
        logger.info("ðŸŽ¯ Clustering optimisÃ©")
        result = super().perform_contract_clustering(df, n_clusters, clustering_type)
        
        self.model_cache[cache_key] = result
        return result
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Statistiques du cache pour monitoring
        """
        return {
            'model_cache_size': len(self.model_cache),
            'model_cache_maxsize': self.model_cache.maxsize,
            'data_cache_size': len(self.data_cache),
            'data_cache_maxsize': self.data_cache.maxsize,
            'cache_hit_info': {
                'model_keys': list(self.model_cache.keys()),
                'data_keys': list(self.data_cache.keys())
            }
        }
    
    def clear_cache(self):
        """
        Vider les caches
        """
        self.model_cache.clear()
        self.data_cache.clear()
        logger.info("ðŸ§¹ Caches vidÃ©s")
    
    def __del__(self):
        """Nettoyage Ã  la destruction"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)
# ml/ml_service.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
import os
from datetime import datetime

from .data_preprocessing import DataPreprocessor
from .models.insurance_models import (
    ClaimsPredictionModel, ProfitabilityModel, RiskClassificationModel,
    ContractClusteringModel, AnomalyDetectionModel, LRCPredictionModel
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MLService:
    """
    Service principal pour toutes les fonctionnalit√©s ML du projet IFRS17
    """
    
    def __init__(self):
        self.preprocessor = DataPreprocessor()
        self.models = {}
        self.model_results = {}
        
    def load_and_preprocess_data(self, data_path: str) -> pd.DataFrame:
        """
        Chargement et pr√©processing des donn√©es
        """
        logger.info(f"üìÅ Chargement des donn√©es depuis {data_path}")
        
        if data_path.endswith('.xlsx'):
            df = pd.read_excel(data_path)
        elif data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        else:
            raise ValueError("Format de fichier non support√©. Utilisez .xlsx ou .csv")
        
        logger.info(f"‚úÖ Donn√©es charg√©es: {df.shape[0]} lignes, {df.shape[1]} colonnes")
        return df
    
    def train_claims_prediction_model(self, df: pd.DataFrame, target_column: str = None, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de pr√©diction des sinistres
        """
        logger.info("üéØ Entra√Ænement du mod√®le de pr√©diction des sinistres")
        
        # Cr√©ation d'une cible synth√©tique si pas fournie
        if target_column is None or target_column not in df.columns:
            # Cr√©er une variable cible bas√©e sur le ratio PPNA/Prime (proxy pour les sinistres)
            if 'MNTPPNA' in df.columns and 'MNTPRNET' in df.columns:
                # Conversion s√©curis√©e des types mixtes
                mntppna_numeric = pd.to_numeric(df['MNTPPNA'], errors='coerce').fillna(0)
                mntprnet_numeric = pd.to_numeric(df['MNTPRNET'], errors='coerce').fillna(1)  # √©viter division par 0
                df['claims_ratio'] = mntppna_numeric / (mntprnet_numeric + 1e-8)
                target_column = 'claims_ratio'
            else:
                raise ValueError("Impossible de cr√©er une variable cible pour les sinistres")
        
        # Preprocessing
        X, y = self.preprocessor.prepare_data_for_training(df, target_column)
        
        # Mod√®le
        model = ClaimsPredictionModel(model_type)
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'claims_prediction_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le de pr√©diction des sinistres entra√Æn√© avec succ√®s")
        return results
    
    def train_profitability_model(self, df: pd.DataFrame, target_column: str = None, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de pr√©diction de rentabilit√©
        """
        logger.info("üí∞ Entra√Ænement du mod√®le de rentabilit√©")
        
        # Cr√©ation d'une cible de rentabilit√©
        if target_column is None or target_column not in df.columns:
            if 'MNTPRNET' in df.columns and 'MNTPPNA' in df.columns:
                # Profit estim√© = Prime - PPNA - Co√ªts estim√©s
                # Conversion s√©curis√©e des types mixtes
                mntprnet_numeric = pd.to_numeric(df['MNTPRNET'], errors='coerce').fillna(0)
                mntppna_numeric = pd.to_numeric(df['MNTPPNA'], errors='coerce').fillna(0)
                estimated_costs = mntprnet_numeric * 0.15  # 15% de co√ªts estim√©s
                df['profitability'] = mntprnet_numeric - mntppna_numeric - estimated_costs
                target_column = 'profitability'
            else:
                raise ValueError("Impossible de cr√©er une variable cible pour la rentabilit√©")
        
        # Preprocessing - ne pas inclure la target dans le preprocessing
        df_copy = df.copy()
        target_data = df_copy[target_column]
        df_copy = df_copy.drop(columns=[target_column])
        
        X, _ = self.preprocessor.prepare_data_for_training(df_copy)
        y = target_data
        
        # Mod√®le
        model = ProfitabilityModel(model_type)
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'profitability_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le de rentabilit√© entra√Æn√© avec succ√®s")
        return results
    
    def train_risk_classification_model(self, df: pd.DataFrame, model_type: str = 'random_forest') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de classification des risques
        """
        logger.info("‚ö†Ô∏è Entra√Ænement du mod√®le de classification des risques")
        
        # Cr√©ation des labels de risque
        model = RiskClassificationModel(model_type)
        risk_labels = model.create_risk_labels(df)
        
        # V√©rification que risk_labels n'est pas None
        if risk_labels is None:
            raise ValueError("Impossible de cr√©er les labels de risque")
        
        # Ajout des labels au dataframe
        df_copy = df.copy()
        df_copy['risk_level'] = risk_labels
        
        # Preprocessing - ne pas inclure la target dans le preprocessing
        target_data = df_copy['risk_level']
        df_copy = df_copy.drop(columns=['risk_level'])
        
        X, _ = self.preprocessor.prepare_data_for_training(df_copy)
        y = target_data
        
        # Filtrage des valeurs non nulles
        valid_indices = y.dropna().index
        X_filtered = X.loc[valid_indices]
        y_filtered = y.loc[valid_indices]
        
        # Encodage des labels de risque
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        y_encoded = le.fit_transform(y_filtered.astype(str))
        
        # Entra√Ænement
        results = model.train(X_filtered, pd.Series(y_encoded, index=X_filtered.index))
        
        # Sauvegarde
        model_key = f'risk_classification_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        self.model_results[model_key]['label_encoder'] = le
        
        logger.info("‚úÖ Mod√®le de classification des risques entra√Æn√© avec succ√®s")
        return results
        self.models[model_key] = model
        self.model_results[model_key] = results
        self.model_results[model_key]['label_encoder'] = le
        
        logger.info("‚úÖ Mod√®le de classification des risques entra√Æn√© avec succ√®s")
        return results
    
    def perform_contract_clustering(self, df: pd.DataFrame, n_clusters: int = 5, clustering_type: str = 'kmeans') -> Dict[str, Any]:
        """
        Clustering des contrats
        """
        logger.info(f"üéØ Clustering des contrats en {n_clusters} groupes")
        
        # Preprocessing pour clustering
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Mod√®le de clustering
        model = ContractClusteringModel(clustering_type)
        model.build_model(n_clusters=n_clusters)
        
        # Clustering
        cluster_labels = model.fit_predict(X)
        
        # Analyse des clusters
        cluster_characteristics = model.get_cluster_characteristics(df)
        
        # Conversion des cl√©s en string pour la s√©rialisation JSON
        cluster_characteristics_clean = {
            str(k): v for k, v in cluster_characteristics.items()
        }
        
        # Sauvegarde
        model_key = f'clustering_{clustering_type}'
        self.models[model_key] = model
        
        results = {
            'cluster_labels': cluster_labels.tolist(),
            'cluster_characteristics': cluster_characteristics_clean,
            'n_clusters': int(len(np.unique(cluster_labels))),
            'cluster_distribution': {str(k): int(v) for k, v in pd.Series(cluster_labels).value_counts().to_dict().items()}
        }
        
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Clustering termin√© avec succ√®s")
        return results
    
    def detect_anomalies(self, df: pd.DataFrame, method: str = 'isolation_forest', contamination: float = 0.1) -> Dict[str, Any]:
        """
        D√©tection d'anomalies dans les contrats
        """
        logger.info(f"üîç D√©tection d'anomalies avec {method}")
        
        # Preprocessing
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Mod√®le de d√©tection d'anomalies
        model = AnomalyDetectionModel(method)
        model.build_model(contamination=contamination)
        
        # D√©tection
        anomaly_labels = model.fit_predict(X)
        anomaly_scores = model.get_anomaly_scores(X)
        
        # Analyse des anomalies
        n_anomalies = np.sum(anomaly_labels == 0)
        anomaly_rate = n_anomalies / len(anomaly_labels)
        
        # Identifier les contrats anormaux
        anomaly_indices = np.where(anomaly_labels == 0)[0]
        anomalous_contracts = df.iloc[anomaly_indices]
        
        # Sauvegarde
        model_key = f'anomaly_detection_{method}'
        self.models[model_key] = model
        
        results = {
            'anomaly_labels': anomaly_labels,
            'anomaly_scores': anomaly_scores,
            'n_anomalies': n_anomalies,
            'anomaly_rate': anomaly_rate,
            'anomalous_contracts': anomalous_contracts.to_dict('records')
        }
        
        self.model_results[model_key] = results
        
        logger.info(f"‚úÖ D√©tection termin√©e: {n_anomalies} anomalies d√©tect√©es ({anomaly_rate:.2%})")
        return results
    
    def train_lrc_prediction_model(self, df: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de pr√©diction LRC (IFRS 17)
        """
        logger.info("üìä Entra√Ænement du mod√®le de pr√©diction LRC")
        
        # Cr√©ation de la variable cible LRC
        model = LRCPredictionModel(model_type)
        lrc_target = model.create_lrc_target(df)
        df['lrc_estimate'] = lrc_target
        
        # Preprocessing
        X, y = self.preprocessor.prepare_data_for_training(df, 'lrc_estimate')
        
        # Entra√Ænement
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'lrc_prediction_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le LRC entra√Æn√© avec succ√®s")
        return results
    
    def predict_with_model(self, model_name: str, df: pd.DataFrame) -> np.ndarray:
        """
        Pr√©diction avec un mod√®le entra√Æn√©
        """
        if model_name not in self.models:
            raise ValueError(f"Mod√®le {model_name} non trouv√©. Mod√®les disponibles: {list(self.models.keys())}")
        
        # Preprocessing des nouvelles donn√©es
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Pr√©diction
        model = self.models[model_name]
        predictions = model.predict(X)
        
        return predictions
    
    def get_model_summary(self) -> Dict[str, Any]:
        """
        R√©sum√© de tous les mod√®les entra√Æn√©s
        """
        summary = {
            'trained_models': list(self.models.keys()),
            'model_performance': {}
        }
        
        for model_name, results in self.model_results.items():
            if 'validation_metrics' in results:
                summary['model_performance'][model_name] = results['validation_metrics']
        
        return summary
    
    def save_all_models(self, save_dir: str = "models"):
        """
        Sauvegarde de tous les mod√®les entra√Æn√©s
        """
        os.makedirs(save_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for model_name, model in self.models.items():
            if hasattr(model, 'save_model'):
                filepath = os.path.join(save_dir, f"{model_name}_{timestamp}.joblib")
                model.save_model(filepath)
        
        logger.info(f"üíæ Tous les mod√®les sauvegard√©s dans {save_dir}")
    
    def generate_ml_insights(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        G√©n√©ration d'insights ML globaux
        """
        insights = {
            'data_overview': {
                'n_contracts': len(df),
                'n_features': len(df.columns),
                'date_range': {
                    'min': df['DEBEFFQUI'].min() if 'DEBEFFQUI' in df.columns else None,
                    'max': df['DEBEFFQUI'].max() if 'DEBEFFQUI' in df.columns else None
                }
            },
            'business_metrics': {},
            'model_recommendations': {}
        }
        
        # M√©triques business
        if 'MNTPRNET' in df.columns:
            insights['business_metrics']['total_premium'] = df['MNTPRNET'].sum()
            insights['business_metrics']['avg_premium'] = df['MNTPRNET'].mean()
            insights['business_metrics']['premium_std'] = df['MNTPRNET'].std()
        
        if 'MNTPPNA' in df.columns:
            insights['business_metrics']['total_ppna'] = df['MNTPPNA'].sum()
            insights['business_metrics']['avg_ppna'] = df['MNTPPNA'].mean()
        
        # Recommandations de mod√®les
        if len(df) > 10000:
            insights['model_recommendations']['preferred_algorithm'] = 'xgboost'
            insights['model_recommendations']['reason'] = 'Dataset volumineux - XGBoost recommand√© pour la performance'
        else:
            insights['model_recommendations']['preferred_algorithm'] = 'random_forest'
            insights['model_recommendations']['reason'] = 'Dataset mod√©r√© - Random Forest recommand√© pour l\'interpr√©tabilit√©'
        
        return insights
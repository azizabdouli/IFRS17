# ml/ml_service.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
import os
from datetime import datetime
from functools import lru_cache
import asyncio
import concurrent.futures
from cachetools import TTLCache

# Import optimis√© avec lazy loading
from .data_preprocessing import OptimizedDataPreprocessor
from .models.insurance_models import (
    ContractClusteringModel,
    ClaimsPredictionModel,
    ProfitabilityModel,
    RiskClassificationModel,
    AnomalyDetectionModel,
    LRCPredictionModel,
    OnerousContractsModel
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedMLService:
    """
    Service ML optimis√© avec cache, lazy loading et traitement asynchrone
    """
    
    def __init__(self, max_workers: int = 4, cache_ttl: int = 3600):
        self.preprocessor = OptimizedDataPreprocessor()
        self.models = {}
        self.model_results = {}
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        
        # Cache pour les mod√®les et r√©sultats
        self.model_cache = TTLCache(maxsize=64, ttl=cache_ttl)
        self.data_cache = TTLCache(maxsize=32, ttl=cache_ttl)
        
        # Import lazy des mod√®les (plus rapide au d√©marrage)
        self._models_imported = False
        
    def _lazy_import_models(self):
        """Import paresseux des mod√®les ML"""
        if not self._models_imported:
            logger.info("üì¶ Import des mod√®les ML...")
            from .models.insurance_models import (
                ClaimsPredictionModel, ProfitabilityModel, RiskClassificationModel,
                ContractClusteringModel, AnomalyDetectionModel, LRCPredictionModel
            )
            self.ClaimsPredictionModel = ClaimsPredictionModel
            self.ProfitabilityModel = ProfitabilityModel
            self.RiskClassificationModel = RiskClassificationModel
            self.ContractClusteringModel = ContractClusteringModel
            self.AnomalyDetectionModel = AnomalyDetectionModel
            self.LRCPredictionModel = LRCPredictionModel
            self._models_imported = True
            logger.info("‚úÖ Mod√®les ML import√©s")
    
    @lru_cache(maxsize=16)
    def _get_file_info(self, data_path: str) -> dict:
        """Cache des infos de fichier"""
        return {
            'size': os.path.getsize(data_path),
            'modified': os.path.getmtime(data_path),
            'extension': os.path.splitext(data_path)[1]
        }
    
    async def load_and_preprocess_data_async(self, data_path: str) -> pd.DataFrame:
        """
        Chargement asynchrone et optimis√© des donn√©es
        """
        file_info = self._get_file_info(data_path)
        cache_key = f"{data_path}_{file_info['modified']}"
        
        # V√©rifier le cache
        if cache_key in self.data_cache:
            logger.info("üìà Donn√©es r√©cup√©r√©es du cache")
            return self.data_cache[cache_key]
        
        logger.info(f"üìÅ Chargement optimis√© des donn√©es ({file_info['size']/1024/1024:.1f}MB)")
        
        # Chargement chunk par chunk pour gros fichiers
        if file_info['size'] > 50 * 1024 * 1024:  # > 50MB
            df = await self._load_large_file_async(data_path, file_info['extension'])
        else:
            df = await self._load_small_file_async(data_path, file_info['extension'])
        
        logger.info(f"‚úÖ Donn√©es charg√©es: {df.shape[0]:,} lignes, {df.shape[1]} colonnes")
        
        # Cache le r√©sultat
        self.data_cache[cache_key] = df
        return df
    
    async def _load_small_file_async(self, data_path: str, extension: str) -> pd.DataFrame:
        """Chargement asynchrone des petits fichiers"""
        loop = asyncio.get_event_loop()
        
        if extension == '.xlsx':
            df = await loop.run_in_executor(self.executor, pd.read_excel, data_path)
        elif extension == '.csv':
            df = await loop.run_in_executor(self.executor, pd.read_csv, data_path)
        else:
            raise ValueError("Format non support√©. Utilisez .xlsx ou .csv")
        
        return df
    
    async def _load_large_file_async(self, data_path: str, extension: str) -> pd.DataFrame:
        """Chargement optimis√© des gros fichiers avec chunks"""
        logger.info("üìä Fichier volumineux d√©tect√© - chargement par chunks")
        
        if extension == '.csv':
            # Chargement par chunks pour CSV
            chunks = []
            loop = asyncio.get_event_loop()
            
            def read_chunk(chunk):
                return chunk
            
            chunk_reader = pd.read_csv(data_path, chunksize=10000)
            for chunk in chunk_reader:
                processed_chunk = await loop.run_in_executor(self.executor, read_chunk, chunk)
                chunks.append(processed_chunk)
            
            return pd.concat(chunks, ignore_index=True)
        else:
            # Pour Excel, chargement normal (pas de chunks support√©s par pandas)
            return await self._load_small_file_async(data_path, extension)
    
    def load_and_preprocess_data(self, data_path: str) -> pd.DataFrame:
        """
        Version synchrone pour compatibilit√©
        """
        return asyncio.run(self.load_and_preprocess_data_async(data_path))
    
    async def train_claims_prediction_model_async(self, df: pd.DataFrame, target_column: str = None, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement asynchrone du mod√®le de pr√©diction des sinistres
        """
        self._lazy_import_models()
        logger.info("üéØ Entra√Ænement optimis√© du mod√®le de pr√©diction des sinistres")
        
        # Cache check
        cache_key = f"claims_{model_type}_{hash(str(df.iloc[:100].values.tobytes()))}"
        if cache_key in self.model_cache:
            logger.info("üìà Mod√®le r√©cup√©r√© du cache")
            return self.model_cache[cache_key]
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor, 
            self._train_claims_prediction_sync, 
            df, target_column, model_type
        )
        
        # Cache le r√©sultat
        self.model_cache[cache_key] = result
        return result
    
    def _train_claims_prediction_sync(self, df: pd.DataFrame, target_column: str, model_type: str) -> Dict[str, Any]:
        """Version synchrone pour l'executor"""
        """
        Entra√Ænement du mod√®le de pr√©diction des sinistres
        """
        logger.info("üéØ Entra√Ænement du mod√®le de pr√©diction des sinistres")
        
        # Cr√©ation d'une cible synth√©tique si pas fournie
        if target_column is None or target_column not in df.columns:
            # Cr√©er une variable cible bas√©e sur le ratio PPNA/Prime (proxy pour les sinistres)
            if 'MNTPPNA' in df.columns and 'MNTPRNET' in df.columns:
                # Conversion s√©curis√©e des types mixtes
                mntppna_numeric = pd.to_numeric(df['MNTPPNA'], errors='coerce').fillna(0)
                mntprnet_numeric = pd.to_numeric(df['MNTPRNET'], errors='coerce').fillna(1)  # √©viter division par 0
                df['claims_ratio'] = mntppna_numeric / (mntprnet_numeric + 1e-8)
                target_column = 'claims_ratio'
            else:
                raise ValueError("Impossible de cr√©er une variable cible pour les sinistres")
        
        # Preprocessing
        X, y = self.preprocessor.prepare_data_for_training(df, target_column)
        
        # Mod√®le
        model = ClaimsPredictionModel(model_type)
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'claims_prediction_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le de pr√©diction des sinistres entra√Æn√© avec succ√®s")
        return results
    
    def train_profitability_model(self, df: pd.DataFrame, target_column: str = None, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de pr√©diction de rentabilit√©
        """
        logger.info("üí∞ Entra√Ænement du mod√®le de rentabilit√©")
        
        # Cr√©ation d'une cible de rentabilit√©
        if target_column is None or target_column not in df.columns:
            if 'MNTPRNET' in df.columns and 'MNTPPNA' in df.columns:
                # Profit estim√© = Prime - PPNA - Co√ªts estim√©s
                # Conversion s√©curis√©e des types mixtes
                mntprnet_numeric = pd.to_numeric(df['MNTPRNET'], errors='coerce').fillna(0)
                mntppna_numeric = pd.to_numeric(df['MNTPPNA'], errors='coerce').fillna(0)
                estimated_costs = mntprnet_numeric * 0.15  # 15% de co√ªts estim√©s
                df['profitability'] = mntprnet_numeric - mntppna_numeric - estimated_costs
                target_column = 'profitability'
            else:
                raise ValueError("Impossible de cr√©er une variable cible pour la rentabilit√©")
        
        # Preprocessing - ne pas inclure la target dans le preprocessing
        df_copy = df.copy()
        target_data = df_copy[target_column]
        df_copy = df_copy.drop(columns=[target_column])
        
        X, _ = self.preprocessor.prepare_data_for_training(df_copy)
        y = target_data
        
        # Mod√®le
        model = ProfitabilityModel(model_type)
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'profitability_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le de rentabilit√© entra√Æn√© avec succ√®s")
        return results
    
    def train_risk_classification_model(self, df: pd.DataFrame, model_type: str = 'random_forest') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de classification des risques
        """
        logger.info("‚ö†Ô∏è Entra√Ænement du mod√®le de classification des risques")
        
        # Cr√©ation des labels de risque
        model = RiskClassificationModel(model_type)
        risk_labels = model.create_risk_labels(df)
        
        # V√©rification que risk_labels n'est pas None
        if risk_labels is None:
            raise ValueError("Impossible de cr√©er les labels de risque")
        
        # Ajout des labels au dataframe
        df_copy = df.copy()
        df_copy['risk_level'] = risk_labels
        
        # Preprocessing - ne pas inclure la target dans le preprocessing
        target_data = df_copy['risk_level']
        df_copy = df_copy.drop(columns=['risk_level'])
        
        X, _ = self.preprocessor.prepare_data_for_training(df_copy)
        y = target_data
        
        # Filtrage des valeurs non nulles
        valid_indices = y.dropna().index
        X_filtered = X.loc[valid_indices]
        y_filtered = y.loc[valid_indices]
        
        # Encodage des labels de risque
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        y_encoded = le.fit_transform(y_filtered.astype(str))
        
        # Entra√Ænement
        results = model.train(X_filtered, pd.Series(y_encoded, index=X_filtered.index))
        
        # Sauvegarde
        model_key = f'risk_classification_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        self.model_results[model_key]['label_encoder'] = le
        
        logger.info("‚úÖ Mod√®le de classification des risques entra√Æn√© avec succ√®s")
        return results
        self.models[model_key] = model
        self.model_results[model_key] = results
        self.model_results[model_key]['label_encoder'] = le
        
        logger.info("‚úÖ Mod√®le de classification des risques entra√Æn√© avec succ√®s")
        return results
    
    def perform_contract_clustering(self, df: pd.DataFrame, n_clusters: int = 5, clustering_type: str = 'kmeans') -> Dict[str, Any]:
        """
        Clustering des contrats
        """
        logger.info(f"üéØ Clustering des contrats en {n_clusters} groupes")
        
        # Preprocessing pour clustering
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Mod√®le de clustering
        model = ContractClusteringModel(clustering_type)
        model.build_model(n_clusters=n_clusters)
        
        # Clustering
        cluster_labels = model.fit_predict(X)
        
        # Analyse des clusters
        cluster_characteristics = model.get_cluster_characteristics(df)
        
        # Conversion des cl√©s en string pour la s√©rialisation JSON
        cluster_characteristics_clean = {
            str(k): v for k, v in cluster_characteristics.items()
        }
        
        # Sauvegarde
        model_key = f'clustering_{clustering_type}'
        self.models[model_key] = model
        
        results = {
            'cluster_labels': cluster_labels.tolist(),
            'cluster_characteristics': cluster_characteristics_clean,
            'n_clusters': int(len(np.unique(cluster_labels))),
            'cluster_distribution': {str(k): int(v) for k, v in pd.Series(cluster_labels).value_counts().to_dict().items()}
        }
        
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Clustering termin√© avec succ√®s")
        return results
    
    def detect_anomalies(self, df: pd.DataFrame, method: str = 'isolation_forest', contamination: float = 0.1) -> Dict[str, Any]:
        """
        D√©tection d'anomalies dans les contrats
        """
        logger.info(f"üîç D√©tection d'anomalies avec {method}")
        
        # Preprocessing
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Mod√®le de d√©tection d'anomalies
        model = AnomalyDetectionModel(method)
        model.build_model(contamination=contamination)
        
        # D√©tection
        anomaly_labels = model.fit_predict(X)
        anomaly_scores = model.get_anomaly_scores(X)
        
        # Analyse des anomalies
        n_anomalies = np.sum(anomaly_labels == 0)
        anomaly_rate = n_anomalies / len(anomaly_labels)
        
        # Identifier les contrats anormaux
        anomaly_indices = np.where(anomaly_labels == 0)[0]
        anomalous_contracts = df.iloc[anomaly_indices]
        
        # Sauvegarde
        model_key = f'anomaly_detection_{method}'
        self.models[model_key] = model
        
        results = {
            'anomaly_labels': anomaly_labels,
            'anomaly_scores': anomaly_scores,
            'n_anomalies': n_anomalies,
            'anomaly_rate': anomaly_rate,
            'anomalous_contracts': anomalous_contracts.to_dict('records')
        }
        
        self.model_results[model_key] = results
        
        logger.info(f"‚úÖ D√©tection termin√©e: {n_anomalies} anomalies d√©tect√©es ({anomaly_rate:.2%})")
        return results
    
    def train_lrc_prediction_model(self, df: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de pr√©diction LRC (IFRS 17)
        """
        logger.info("üìä Entra√Ænement du mod√®le de pr√©diction LRC")
        
        # Cr√©ation de la variable cible LRC
        model = LRCPredictionModel(model_type)
        lrc_target = model.create_lrc_target(df)
        df['lrc_estimate'] = lrc_target
        
        # Preprocessing
        X, y = self.preprocessor.prepare_data_for_training(df, 'lrc_estimate')
        
        # Entra√Ænement
        results = model.train(X, y)
        
        # Sauvegarde
        model_key = f'lrc_prediction_{model_type}'
        self.models[model_key] = model
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Pr√©diction LRC termin√©e avec succ√®s")
        return results
    
    def train_onerous_contracts_model(self, df: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le de d√©tection des contrats on√©reux
        """
        logger.info(f"üéØ Entra√Ænement mod√®le contrats on√©reux avec {model_type}")
        
        # Preprocessing
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Mod√®le sp√©cialis√© contrats on√©reux
        model = OnerousContractsModel(model_type)
        model.build_model()
        
        # Pr√©paration des features sp√©cifiques
        X_enhanced = model.prepare_features(df)
        X_processed, _ = self.preprocessor.prepare_data_for_training(X_enhanced)
        
        # Cr√©ation de la cible
        y_onerous = model.create_onerous_target(df)
        
        # Entra√Ænement
        model.train(X_processed, y_onerous)
        
        # √âvaluation
        from sklearn.model_selection import cross_val_score
        from sklearn.metrics import classification_report, confusion_matrix
        
        cv_scores = cross_val_score(model.model, X_processed, y_onerous, cv=5, scoring='accuracy')
        predictions = model.predict(X_processed)
        
        # Analyse des patterns
        onerous_analysis = model.analyze_onerous_patterns(df, predictions)
        
        # Insights d√©taill√©s
        probabilities = model.predict_proba(X_processed)
        insights = model.get_onerous_insights(df, predictions, probabilities)
        
        # Sauvegarde
        model_key = f'onerous_contracts_{model_type}'
        self.models[model_key] = model
        
        results = {
            'model_type': model_type,
            'cv_accuracy': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'predictions': predictions.tolist(),
            'probabilities': probabilities[:, 1].tolist(),  # Probabilit√© d'√™tre on√©reux
            'onerous_analysis': onerous_analysis,
            'insights': insights,
            'feature_importance': model.feature_importance,
            'performance_metrics': {
                'accuracy': cv_scores.mean(),
                'std_deviation': cv_scores.std(),
                'onerous_rate': np.mean(predictions),
                'high_risk_count': len(insights['high_risk_contracts'])
            }
        }
        
        self.model_results[model_key] = results
        
        logger.info("‚úÖ Mod√®le contrats on√©reux entra√Æn√© avec succ√®s")
        logger.info(f"üìä Taux de contrats on√©reux: {np.mean(predictions):.1%}")
        logger.info(f"üéØ Pr√©cision: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})")
        
        return results
    
    def predict_onerous_contracts(self, df: pd.DataFrame, model_type: str = 'xgboost') -> Dict[str, Any]:
        """
        Pr√©diction des contrats on√©reux
        """
        model_key = f'onerous_contracts_{model_type}'
        
        if model_key not in self.models:
            raise ValueError(f"Mod√®le {model_key} non trouv√©. Entra√Ænez d'abord le mod√®le.")
        
        model = self.models[model_key]
        
        # Pr√©paration des donn√©es
        X_enhanced = model.prepare_features(df)
        X_processed, _ = self.preprocessor.prepare_data_for_training(X_enhanced)
        
        # Pr√©dictions
        predictions = model.predict(X_processed)
        probabilities = model.predict_proba(X_processed)
        
        # Analyse
        onerous_analysis = model.analyze_onerous_patterns(df, predictions)
        insights = model.get_onerous_insights(df, predictions, probabilities)
        
        return {
            'predictions': predictions.tolist(),
            'probabilities': probabilities[:, 1].tolist(),
            'onerous_analysis': onerous_analysis,
            'insights': insights,
            'model_used': model_type
        }
        return results
    
    def predict_with_model(self, model_name: str, df: pd.DataFrame) -> np.ndarray:
        """
        Pr√©diction avec un mod√®le entra√Æn√©
        """
        if model_name not in self.models:
            raise ValueError(f"Mod√®le {model_name} non trouv√©. Mod√®les disponibles: {list(self.models.keys())}")
        
        # Preprocessing des nouvelles donn√©es
        X, _ = self.preprocessor.prepare_data_for_training(df)
        
        # Pr√©diction
        model = self.models[model_name]
        predictions = model.predict(X)
        
        return predictions
    
    def get_model_summary(self) -> Dict[str, Any]:
        """
        R√©sum√© de tous les mod√®les entra√Æn√©s
        """
        summary = {
            'trained_models': list(self.models.keys()),
            'model_performance': {}
        }
        
        for model_name, results in self.model_results.items():
            if 'validation_metrics' in results:
                summary['model_performance'][model_name] = results['validation_metrics']
        
        return summary
    
    def save_all_models(self, save_dir: str = "models"):
        """
        Sauvegarde de tous les mod√®les entra√Æn√©s
        """
        os.makedirs(save_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for model_name, model in self.models.items():
            if hasattr(model, 'save_model'):
                filepath = os.path.join(save_dir, f"{model_name}_{timestamp}.joblib")
                model.save_model(filepath)
        
        logger.info(f"üíæ Tous les mod√®les sauvegard√©s dans {save_dir}")
    
    def generate_ml_insights(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        G√©n√©ration d'insights ML globaux
        """
        insights = {
            'data_overview': {
                'n_contracts': len(df),
                'n_features': len(df.columns),
                'date_range': {
                    'min': df['DEBEFFQUI'].min() if 'DEBEFFQUI' in df.columns else None,
                    'max': df['DEBEFFQUI'].max() if 'DEBEFFQUI' in df.columns else None
                }
            },
            'business_metrics': {},
            'model_recommendations': {}
        }
        
        # M√©triques business
        if 'MNTPRNET' in df.columns:
            insights['business_metrics']['total_premium'] = df['MNTPRNET'].sum()
            insights['business_metrics']['avg_premium'] = df['MNTPRNET'].mean()
            insights['business_metrics']['premium_std'] = df['MNTPRNET'].std()
        
        if 'MNTPPNA' in df.columns:
            insights['business_metrics']['total_ppna'] = df['MNTPPNA'].sum()
            insights['business_metrics']['avg_ppna'] = df['MNTPPNA'].mean()
        
        # Recommandations de mod√®les
        if len(df) > 10000:
            insights['model_recommendations']['preferred_algorithm'] = 'xgboost'
            insights['model_recommendations']['reason'] = 'Dataset volumineux - XGBoost recommand√© pour la performance'
        else:
            insights['model_recommendations']['preferred_algorithm'] = 'random_forest'
            insights['model_recommendations']['reason'] = 'Dataset mod√©r√© - Random Forest recommand√© pour l\'interpr√©tabilit√©'
        
        return insights
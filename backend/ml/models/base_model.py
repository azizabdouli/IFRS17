# ml/models/base_model.py

import pandas as pd
import numpy as np
from abc import ABC, abstractmethod
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, confusion_matrix
import joblib
import logging
from typing import Dict, Any, Tuple, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseMLModel(ABC):
    """
    Classe de base pour tous les mod√®les ML du projet IFRS17
    """
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = None
        self.is_trained = False
        self.feature_names = None
        self.model_type = None  # 'regression' ou 'classification'
        
    @abstractmethod
    def build_model(self, **kwargs):
        """Construction du mod√®le"""
        pass
    
    def train(self, X: pd.DataFrame, y: pd.Series, validation_split: float = 0.2, **kwargs) -> Dict[str, Any]:
        """
        Entra√Ænement du mod√®le avec validation
        """
        logger.info(f"üöÄ D√©but de l'entra√Ænement du mod√®le {self.model_name}")
        
        # V√©rifications de s√©curit√© des donn√©es
        if X is None:
            raise ValueError("‚ùå Les donn√©es d'entr√©e X sont None")
        if y is None:
            raise ValueError("‚ùå Les donn√©es cible y sont None")
        if len(X) == 0:
            raise ValueError("‚ùå Le DataFrame X est vide")
        if len(y) == 0:
            raise ValueError("‚ùå La s√©rie y est vide")
        if len(X) != len(y):
            raise ValueError(f"‚ùå Tailles incompatibles: X={len(X)}, y={len(y)}")
        
        logger.info(f"‚úÖ Validation des donn√©es: X={X.shape}, y={len(y)}")
        
        # S'assurer que les index sont align√©s et continus
        X = X.reset_index(drop=True)
        y = y.reset_index(drop=True)
        
        # Nettoyage du target pour XGBoost
        mask_valid = self._clean_target(y)
        n_removed = (~mask_valid).sum()
        
        if n_removed > 0:
            logger.warning(f"‚ö†Ô∏è Suppression de {n_removed} valeurs probl√©matiques du target")
            # Appliquer le masque pour synchroniser X et y
            X = X[mask_valid].reset_index(drop=True)
            y = y[mask_valid].reset_index(drop=True)
            logger.info(f"‚úÖ Donn√©es synchronis√©es apr√®s nettoyage: X={X.shape}, y={len(y)}")
        else:
            logger.info("‚úÖ Aucune valeur probl√©matique d√©tect√©e dans le target")
        
        # Sauvegarde des noms de features
        self.feature_names = X.columns.tolist()
        
        # Split train/validation - gestion robuste de la stratification
        try:
            if self.model_type == 'classification' and len(y.unique()) > 1:
                # V√©rifier si la stratification est possible
                min_class_count = y.value_counts().min()
                if min_class_count >= 2:
                    X_train, X_val, y_train, y_val = train_test_split(
                        X, y, test_size=validation_split, random_state=42, stratify=y
                    )
                else:
                    X_train, X_val, y_train, y_val = train_test_split(
                        X, y, test_size=validation_split, random_state=42
                    )
            else:
                X_train, X_val, y_train, y_val = train_test_split(
                    X, y, test_size=validation_split, random_state=42
                )
        except Exception:
            # Fallback sans stratification
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=validation_split, random_state=42
            )
        
        # Construction du mod√®le si pas encore fait
        if self.model is None:
            self.build_model(**kwargs)
        
        # Entra√Ænement
        self.model.fit(X_train, y_train)
        self.is_trained = True
        
        # Validation
        train_score = self.evaluate(X_train, y_train)
        val_score = self.evaluate(X_val, y_val)
        
        # Cross-validation avec gestion d'erreurs
        try:
            cv_scores = cross_val_score(self.model, X, y, cv=min(5, len(X)//2))
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
        except Exception:
            cv_mean = val_score.get('r2', val_score.get('accuracy', 0))
            cv_std = 0
        
        results = {
            'train_metrics': train_score,
            'validation_metrics': val_score,
            'cv_mean': cv_mean,
            'cv_std': cv_std,
            'feature_importance': self.get_feature_importance()
        }
        
        logger.info(f"‚úÖ Entra√Ænement termin√© pour {self.model_name}")
        logger.info(f"üìä Score validation: {val_score}")
        
        return results
    
    def _clean_target(self, y: pd.Series) -> pd.Series:
        """
        Nettoie les valeurs cibles pour les rendre compatibles avec XGBoost
        Supprime les NaN, infinity, et valeurs aberrantes
        G√®re √† la fois les targets num√©riques (r√©gression) et cat√©goriels (classification)
        """
        import numpy as np
        import pandas as pd
        
        logger.info(f"üßπ Nettoyage du target - Taille initiale: {len(y)}")
        
        # Cr√©er une copie pour √©viter de modifier l'original
        y_clean = y.copy()
        
        # D√©tecter si le target est cat√©goriel ou num√©rique
        is_categorical = (y_clean.dtype == 'object' or 
                         isinstance(y_clean.dtype, pd.CategoricalDtype) or
                         all(isinstance(val, str) for val in y_clean.dropna().head(10)))
        
        if is_categorical:
            logger.info("üè∑Ô∏è Target cat√©goriel d√©tect√© - nettoyage simplifi√©")
            # Pour les targets cat√©goriels, on ne supprime que les NaN
            nan_count = y_clean.isna().sum()
            logger.info(f"üìä NaN trouv√©s: {nan_count}")
            
            mask_valid = ~y_clean.isna()
            total_removed = (~mask_valid).sum()
            total_remaining = mask_valid.sum()
            
        else:
            logger.info("üî¢ Target num√©rique d√©tect√© - nettoyage complet")
            # Pour les targets num√©riques, nettoyage complet
            nan_count = y_clean.isna().sum()
            inf_count = np.isinf(y_clean.astype(float)).sum() if y_clean.dtype != 'object' else 0
            
            logger.info(f"üìä NaN trouv√©s: {nan_count}, Infinity trouv√©s: {inf_count}")
            
            # 1. Convertir en num√©rique si ce n'est pas d√©j√† fait
            if y_clean.dtype == 'object':
                y_clean = pd.to_numeric(y_clean, errors='coerce')
                logger.info("üîÑ Conversion object -> numeric effectu√©e")
            
            # 2. Identifier les valeurs probl√©matiques
            mask_valid = True
            
            # NaN values
            mask_nan = y_clean.isna()
            mask_valid = mask_valid & ~mask_nan
            
            # Infinity values
            mask_inf = np.isinf(y_clean)
            mask_valid = mask_valid & ~mask_inf
            
            # Valeurs extr√™mes (probablement des erreurs)
            if len(y_clean[~mask_nan & ~mask_inf]) > 0:
                q1 = y_clean[~mask_nan & ~mask_inf].quantile(0.01)
                q99 = y_clean[~mask_nan & ~mask_inf].quantile(0.99)
                iqr = y_clean[~mask_nan & ~mask_inf].quantile(0.75) - y_clean[~mask_nan & ~mask_inf].quantile(0.25)
                
                # Limites pour les outliers extr√™mes
                lower_bound = q1 - 3 * iqr
                upper_bound = q99 + 3 * iqr
                
                mask_outliers = (y_clean < lower_bound) | (y_clean > upper_bound)
                mask_valid = mask_valid & ~mask_outliers
                
                outliers_count = mask_outliers.sum()
                logger.info(f"üéØ Outliers d√©tect√©s: {outliers_count} (< {lower_bound:.2f} ou > {upper_bound:.2f})")
            
            # 3. Compter les valeurs supprim√©es
            total_removed = (~mask_valid).sum()
            total_remaining = mask_valid.sum()
        
        logger.info(f"‚ùå Valeurs supprim√©es: {total_removed}")
        logger.info(f"‚úÖ Valeurs conserv√©es: {total_remaining}")
        
        # 4. V√©rifier qu'il reste assez de donn√©es
        min_required = max(2, min(10, len(y) // 2))  # Au moins 2, maximum 10, ou la moiti√© des donn√©es
        if total_remaining < min_required:
            logger.warning(f"‚ö†Ô∏è Tr√®s peu de donn√©es valides restantes: {total_remaining}")
            if total_remaining < 2:
                raise ValueError(f"Pas assez de donn√©es valides apr√®s nettoyage: {total_remaining} < 2")
        
        # 5. Retourner les indices valides pour synchronisation avec X
        return mask_valid
        return mask_valid
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Pr√©diction
        """
        if not self.is_trained:
            raise ValueError(f"Le mod√®le {self.model_name} n'est pas encore entra√Æn√©")
        
        # V√©rifier que les features correspondent
        if self.feature_names and X.columns.tolist() != self.feature_names:
            logger.warning("‚ö†Ô∏è Les features ne correspondent pas exactement au mod√®le entra√Æn√©")
            # R√©organiser les colonnes pour correspondre
            X = X.reindex(columns=self.feature_names, fill_value=0)
        
        return self.model.predict(X)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        Pr√©diction de probabilit√©s (pour classification)
        """
        if not self.is_trained:
            raise ValueError(f"Le mod√®le {self.model_name} n'est pas encore entra√Æn√©")
        
        if self.model_type != 'classification':
            raise ValueError("predict_proba n'est disponible que pour les mod√®les de classification")
        
        if not hasattr(self.model, 'predict_proba'):
            raise ValueError("Ce mod√®le ne supporte pas predict_proba")
        
        return self.model.predict_proba(X)
    
    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        √âvaluation du mod√®le
        """
        predictions = self.predict(X)
        
        if self.model_type == 'regression':
            return {
                'mse': mean_squared_error(y, predictions),
                'mae': mean_absolute_error(y, predictions),
                'rmse': np.sqrt(mean_squared_error(y, predictions)),
                'r2': r2_score(y, predictions)
            }
        elif self.model_type == 'classification':
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            return {
                'accuracy': accuracy_score(y, predictions),
                'precision': precision_score(y, predictions, average='weighted'),
                'recall': recall_score(y, predictions, average='weighted'),
                'f1': f1_score(y, predictions, average='weighted')
            }
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        Importance des features
        """
        if not self.is_trained:
            return None
        
        if hasattr(self.model, 'feature_importances_'):
            importance_dict = dict(zip(self.feature_names, self.model.feature_importances_))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
        elif hasattr(self.model, 'coef_'):
            # Pour les mod√®les lin√©aires
            importance_dict = dict(zip(self.feature_names, np.abs(self.model.coef_)))
            return dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
        else:
            return None
    
    def save_model(self, filepath: str):
        """
        Sauvegarde du mod√®le
        """
        if not self.is_trained:
            raise ValueError("Le mod√®le doit √™tre entra√Æn√© avant d'√™tre sauvegard√©")
        
        model_data = {
            'model': self.model,
            'model_name': self.model_name,
            'model_type': self.model_type,
            'feature_names': self.feature_names,
            'is_trained': self.is_trained
        }
        
        joblib.dump(model_data, filepath)
        logger.info(f"üíæ Mod√®le sauvegard√© : {filepath}")
    
    def load_model(self, filepath: str):
        """
        Chargement du mod√®le
        """
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.model_name = model_data['model_name']
        self.model_type = model_data['model_type']
        self.feature_names = model_data['feature_names']
        self.is_trained = model_data['is_trained']
        
        logger.info(f"üìÇ Mod√®le charg√© : {filepath}")
# backend/ai/ifrs17_ai_assistant.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
import json
from datetime import datetime
import asyncio
from functools import lru_cache

# Import pour LLM local ou API
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = logging.getLogger(__name__)

class IFRS17AIAssistant:
    """
    Assistant IA spÃ©cialisÃ© IFRS17 avec capacitÃ©s conversationnelles
    et analyse intelligente des donnÃ©es d'assurance
    """
    
    def __init__(self):
        self.conversation_history = []
        self.context_memory = {}
        self.ifrs17_knowledge = self._load_ifrs17_knowledge()
        
        # Initialiser le modÃ¨le IA si disponible
        self.llm_pipeline = None
        if TRANSFORMERS_AVAILABLE:
            self._initialize_llm()
        
        logger.info("ðŸ¤– Assistant IA IFRS17 initialisÃ©")
    
    def _initialize_llm(self):
        """Initialise le modÃ¨le de langage local"""
        try:
            # Utiliser un modÃ¨le lÃ©ger pour commencer
            model_name = "microsoft/DialoGPT-medium"
            self.llm_pipeline = pipeline(
                "conversational",
                model=model_name,
                tokenizer=model_name
            )
            logger.info(f"âœ… ModÃ¨le LLM initialisÃ©: {model_name}")
        except Exception as e:
            logger.warning(f"âš ï¸ Impossible de charger le LLM local: {e}")
    
    def _load_ifrs17_knowledge(self) -> Dict[str, Any]:
        """Base de connaissances IFRS17 spÃ©cialisÃ©e"""
        return {
            "definitions": {
                "PAA": "Premium Allocation Approach - Approche d'allocation des primes",
                "CSM": "Contractual Service Margin - Marge de service contractuel",
                "LRC": "Liability for Remaining Coverage - Passif pour couverture restante",
                "LIC": "Liability for Incurred Claims - Passif pour sinistres survenus",
                "FCF": "Fulfilment Cash Flows - Flux de trÃ©sorerie d'exÃ©cution"
            },
            "calculations": {
                "profitability": "RentabilitÃ© = (Primes - Sinistres - Frais) / Primes",
                "loss_ratio": "Ratio sinistres = Sinistres / Primes acquises",
                "combined_ratio": "Ratio combinÃ© = (Sinistres + Frais) / Primes"
            },
            "models": {
                "profitability": {"performance": "RÂ² = 0.964", "usage": "Analyse rentabilitÃ©"},
                "risk_classification": {"performance": "85%+ accuracy", "usage": "Classification risques"},
                "claims_prediction": {"performance": "RÂ² = 0.732", "usage": "PrÃ©diction sinistres"},
                "lrc_prediction": {"performance": "RÂ² = 0.937", "usage": "PrÃ©diction LRC"}
            }
        }
    
    async def process_query(self, user_query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Traite une requÃªte utilisateur avec intelligence contextuelle
        """
        # Ajouter Ã  l'historique
        self.conversation_history.append({
            "timestamp": datetime.now(),
            "user_query": user_query,
            "context": context or {}
        })
        
        # Analyser l'intention
        intent = self._analyze_intent(user_query)
        
        # GÃ©nÃ©rer la rÃ©ponse
        response = await self._generate_response(user_query, intent, context)
        
        # Ajouter Ã  l'historique
        self.conversation_history[-1]["ai_response"] = response
        
        return response
    
    def _analyze_intent(self, query: str) -> Dict[str, Any]:
        """Analyse l'intention de la requÃªte utilisateur"""
        query_lower = query.lower()
        
        intent = {
            "type": "general",
            "confidence": 0.5,
            "keywords": [],
            "data_needed": False,
            "action": None
        }
        
        # Mots-clÃ©s IFRS17
        ifrs17_keywords = ["paa", "csm", "lrc", "lic", "fcf", "ifrs17", "prime", "sinistre"]
        found_keywords = [kw for kw in ifrs17_keywords if kw in query_lower]
        intent["keywords"] = found_keywords
        
        # DÃ©tection d'intention
        if any(word in query_lower for word in ["analyse", "analyser", "donnÃ©es"]):
            intent["type"] = "analysis"
            intent["confidence"] = 0.8
            intent["data_needed"] = True
        elif any(word in query_lower for word in ["prÃ©diction", "prÃ©voir", "modÃ¨le"]):
            intent["type"] = "prediction"
            intent["confidence"] = 0.9
            intent["action"] = "run_model"
        elif any(word in query_lower for word in ["clustering", "groupe", "segmentation"]):
            intent["type"] = "clustering"
            intent["confidence"] = 0.85
            intent["action"] = "clustering"
        elif any(word in query_lower for word in ["dÃ©finition", "qu'est-ce", "expliquer"]):
            intent["type"] = "explanation"
            intent["confidence"] = 0.9
        elif any(word in query_lower for word in ["performance", "rÃ©sultat", "mÃ©trique"]):
            intent["type"] = "performance"
            intent["confidence"] = 0.8
        
        return intent
    
    async def _generate_response(self, query: str, intent: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """GÃ©nÃ¨re une rÃ©ponse intelligente basÃ©e sur l'intention"""
        
        response = {
            "text": "",
            "suggestions": [],
            "actions": [],
            "data_insights": None,
            "confidence": intent["confidence"]
        }
        
        if intent["type"] == "explanation":
            response = await self._handle_explanation(query, intent)
        elif intent["type"] == "analysis":
            response = await self._handle_analysis(query, context)
        elif intent["type"] == "prediction":
            response = await self._handle_prediction(query, context)
        elif intent["type"] == "performance":
            response = await self._handle_performance(query)
        else:
            response = await self._handle_general(query, intent)
        
        return response
    
    async def _handle_explanation(self, query: str, intent: Dict[str, Any]) -> Dict[str, Any]:
        """GÃ¨re les demandes d'explication"""
        explanations = []
        
        for keyword in intent["keywords"]:
            if keyword in self.ifrs17_knowledge["definitions"]:
                explanations.append(f"**{keyword.upper()}**: {self.ifrs17_knowledge['definitions'][keyword]}")
        
        if not explanations:
            explanations = ["Je peux vous expliquer les concepts IFRS17 : PAA, CSM, LRC, LIC, FCF..."]
        
        return {
            "text": f"ðŸ“š **Explications IFRS17:**\n\n" + "\n\n".join(explanations),
            "suggestions": ["Expliquer PAA", "DÃ©finir CSM", "Calculer la rentabilitÃ©"],
            "actions": [],
            "confidence": 0.9
        }
    
    async def _handle_analysis(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """GÃ¨re les demandes d'analyse de donnÃ©es"""
        if not context or "data" not in context:
            return {
                "text": "ðŸ” **Analyse de donnÃ©es requise**\n\nPour effectuer une analyse, veuillez d'abord charger vos donnÃ©es IFRS17.",
                "suggestions": ["Charger des donnÃ©es", "Voir exemple d'analyse"],
                "actions": ["upload_data"],
                "confidence": 0.8
            }
        
        # Analyse des donnÃ©es si disponibles
        data_insights = self._analyze_data(context["data"])
        
        return {
            "text": f"ðŸ“Š **Analyse de vos donnÃ©es IFRS17:**\n\n{data_insights['summary']}",
            "suggestions": ["Clustering", "PrÃ©dictions", "DÃ©tection d'anomalies"],
            "actions": ["run_clustering", "run_predictions"],
            "data_insights": data_insights,
            "confidence": 0.9
        }
    
    async def _handle_prediction(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """GÃ¨re les demandes de prÃ©diction"""
        available_models = list(self.ifrs17_knowledge["models"].keys())
        
        text = "ðŸ”® **ModÃ¨les de prÃ©diction disponibles:**\n\n"
        for model, info in self.ifrs17_knowledge["models"].items():
            text += f"â€¢ **{model.title()}**: {info['usage']} (Performance: {info['performance']})\n"
        
        return {
            "text": text,
            "suggestions": [f"PrÃ©diction {model}" for model in available_models],
            "actions": ["select_model", "run_prediction"],
            "confidence": 0.85
        }
    
    async def _handle_performance(self, query: str) -> Dict[str, Any]:
        """GÃ¨re les demandes de performance"""
        perf_text = "ðŸ“ˆ **Performance des modÃ¨les IFRS17:**\n\n"
        
        for model, info in self.ifrs17_knowledge["models"].items():
            perf_text += f"â€¢ **{model.replace('_', ' ').title()}**: {info['performance']}\n"
        
        perf_text += "\nðŸš€ **Optimisations systÃ¨me:**\n"
        perf_text += "â€¢ Traitement: 1,171,318 lignes/sec\n"
        perf_text += "â€¢ Cache: 90%+ hit rate\n"
        perf_text += "â€¢ MÃ©moire: <3MB utilisation"
        
        return {
            "text": perf_text,
            "suggestions": ["Tests performance", "Optimiser modÃ¨les"],
            "actions": ["run_performance_test"],
            "confidence": 0.9
        }
    
    async def _handle_general(self, query: str, intent: Dict[str, Any]) -> Dict[str, Any]:
        """GÃ¨re les requÃªtes gÃ©nÃ©rales"""
        if self.llm_pipeline and TRANSFORMERS_AVAILABLE:
            try:
                # Utiliser le LLM local
                result = self.llm_pipeline(query)
                ai_text = result.generated_responses[0] if result.generated_responses else query
            except:
                ai_text = self._generate_template_response(query, intent)
        else:
            ai_text = self._generate_template_response(query, intent)
        
        return {
            "text": f"ðŸ¤– {ai_text}",
            "suggestions": ["Analyser donnÃ©es", "PrÃ©dictions", "Performance"],
            "actions": ["general_help"],
            "confidence": intent["confidence"]
        }
    
    def _generate_template_response(self, query: str, intent: Dict[str, Any]) -> str:
        """GÃ©nÃ¨re une rÃ©ponse template intelligente"""
        templates = [
            "Je suis votre assistant IA spÃ©cialisÃ© IFRS17. Comment puis-je vous aider avec l'analyse de vos contrats d'assurance ?",
            "Excellente question ! Je peux vous aider avec l'analyse IFRS17, les prÃ©dictions ML, et l'optimisation de performance.",
            "En tant qu'expert IA IFRS17, je peux analyser vos donnÃ©es, expliquer les concepts, et recommander des actions."
        ]
        
        if intent["keywords"]:
            return f"Je vois que vous vous intÃ©ressez Ã  : {', '.join(intent['keywords'])}. {templates[0]}"
        
        return templates[0]
    
    def _analyze_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse intelligente des donnÃ©es IFRS17"""
        try:
            insights = {
                "summary": "",
                "statistics": {},
                "recommendations": [],
                "anomalies": []
            }
            
            # Statistiques de base
            insights["statistics"] = {
                "total_contracts": len(df),
                "total_premium": df.get("prime_brute", pd.Series()).sum() if "prime_brute" in df.columns else 0,
                "columns_count": len(df.columns),
                "missing_data": df.isnull().sum().sum()
            }
            
            # GÃ©nÃ©ration du rÃ©sumÃ©
            insights["summary"] = f"""
ðŸ“Š **{insights['statistics']['total_contracts']:,} contrats** analysÃ©s
ðŸ’° **Prime totale**: {insights['statistics']['total_premium']:,.0f}â‚¬
ðŸ“‹ **{insights['statistics']['columns_count']} colonnes** de donnÃ©es
âš ï¸ **{insights['statistics']['missing_data']} valeurs manquantes**
            """.strip()
            
            # Recommandations intelligentes
            if insights['statistics']['missing_data'] > 0:
                insights["recommendations"].append("ðŸ”§ Nettoyer les donnÃ©es manquantes")
            
            if insights['statistics']['total_contracts'] > 10000:
                insights["recommendations"].append("ðŸš€ Utiliser le clustering pour segmenter")
            
            insights["recommendations"].append("ðŸ“ˆ Lancer les prÃ©dictions ML")
            
            return insights
            
        except Exception as e:
            logger.error(f"Erreur analyse donnÃ©es: {e}")
            return {"summary": "Erreur lors de l'analyse", "statistics": {}, "recommendations": []}
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Retourne l'historique de conversation"""
        return self.conversation_history[-10:]  # DerniÃ¨res 10 interactions
    
    def clear_history(self):
        """Vide l'historique de conversation"""
        self.conversation_history.clear()
        logger.info("ðŸ§¹ Historique de conversation vidÃ©")
    
    @lru_cache(maxsize=32)
    def get_quick_help(self, topic: str = "general") -> str:
        """Aide rapide mise en cache"""
        help_topics = {
            "general": "ðŸ’¡ **Aide**: Posez-moi des questions sur IFRS17, l'analyse de donnÃ©es, ou les prÃ©dictions ML.",
            "upload": "ðŸ“¤ **Upload**: Glissez votre fichier Excel/CSV avec les colonnes : NUMQUITT, prime_brute, date_effet...",
            "models": "ðŸ¤– **ModÃ¨les**: 4 modÃ¨les ML disponibles pour rentabilitÃ©, risques, sinistres, et LRC.",
            "performance": "âš¡ **Performance**: Tests automatisÃ©s avec 1M+ lignes/sec et cache intelligent."
        }
        return help_topics.get(topic, help_topics["general"])
# backend/ai/predictive_ai_service.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
from datetime import datetime, timedelta
import asyncio
from functools import lru_cache
import json

# ML avanc√©
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import joblib

logger = logging.getLogger(__name__)

class PredictiveAIService:
    """
    Service IA pr√©dictif avanc√© pour IFRS17
    avec auto-ML et recommandations intelligentes
    """
    
    def __init__(self):
        self.models = {}
        self.predictions_cache = {}
        self.ai_insights = {}
        self.auto_ml_config = self._init_auto_ml_config()
        
        logger.info("üß† Service IA Pr√©dictif initialis√©")
    
    def _init_auto_ml_config(self) -> Dict[str, Any]:
        """Configuration Auto-ML"""
        return {
            "algorithms": {
                "regression": ["random_forest", "xgboost", "lightgbm"],
                "classification": ["random_forest", "svm", "neural_network"],
                "clustering": ["kmeans", "dbscan", "hierarchical"]
            },
            "auto_optimize": True,
            "cross_validation": 5,
            "performance_threshold": {
                "regression": 0.7,  # R¬≤
                "classification": 0.8,  # Accuracy
                "clustering": 0.6  # Silhouette score
            }
        }
    
    async def auto_analyze_dataset(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyse automatique intelligente du dataset
        avec recommandations IA
        """
        logger.info("üîç Analyse automatique IA du dataset...")
        
        analysis = {
            "data_quality": await self._assess_data_quality(df),
            "feature_importance": await self._analyze_feature_importance(df),
            "patterns": await self._detect_patterns(df),
            "recommendations": [],
            "ai_suggestions": []
        }
        
        # G√©n√©rer recommandations intelligentes
        analysis["recommendations"] = self._generate_smart_recommendations(analysis)
        analysis["ai_suggestions"] = self._generate_ai_suggestions(df, analysis)
        
        return analysis
    
    async def _assess_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """√âvaluation de la qualit√© des donn√©es avec IA"""
        quality = {
            "score": 0.0,
            "issues": [],
            "strengths": [],
            "completeness": 0.0,
            "consistency": 0.0,
            "validity": 0.0
        }
        
        # Compl√©tude
        missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
        quality["completeness"] = max(0, 1 - missing_ratio)
        
        # Coh√©rence
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            # D√©tecter les outliers
            outlier_ratio = 0
            for col in numeric_cols[:5]:  # Limiter pour performance
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]
                outlier_ratio += len(outliers) / len(df)
            
            quality["consistency"] = max(0, 1 - outlier_ratio / len(numeric_cols))
        else:
            quality["consistency"] = 0.8
        
        # Validit√© (r√®gles m√©tier IFRS17)
        validity_score = self._check_ifrs17_validity(df)
        quality["validity"] = validity_score
        
        # Score global
        quality["score"] = (quality["completeness"] + quality["consistency"] + quality["validity"]) / 3
        
        # Issues et forces
        if quality["completeness"] < 0.9:
            quality["issues"].append(f"Donn√©es manquantes: {missing_ratio:.1%}")
        else:
            quality["strengths"].append("Compl√©tude excellente")
        
        if quality["consistency"] < 0.8:
            quality["issues"].append("Outliers d√©tect√©s")
        else:
            quality["strengths"].append("Donn√©es coh√©rentes")
        
        return quality
    
    def _check_ifrs17_validity(self, df: pd.DataFrame) -> float:
        """V√©rification des r√®gles m√©tier IFRS17"""
        validity_checks = []
        
        # V√©rifier les primes positives
        if "prime_brute" in df.columns:
            positive_primes = (df["prime_brute"] > 0).sum() / len(df)
            validity_checks.append(positive_primes)
        
        # V√©rifier les dates coh√©rentes
        if "date_effet" in df.columns:
            try:
                dates = pd.to_datetime(df["date_effet"], errors='coerce')
                valid_dates = dates.notna().sum() / len(df)
                validity_checks.append(valid_dates)
            except:
                validity_checks.append(0.5)
        
        # V√©rifier la dur√©e positive
        if "duree_mois" in df.columns:
            positive_duration = (df["duree_mois"] > 0).sum() / len(df)
            validity_checks.append(positive_duration)
        
        return np.mean(validity_checks) if validity_checks else 0.8
    
    async def _analyze_feature_importance(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analyse de l'importance des features avec ML"""
        try:
            # Pr√©parer les donn√©es num√©riques
            numeric_df = df.select_dtypes(include=[np.number]).fillna(0)
            
            if len(numeric_df.columns) < 2:
                return {"importance": {}, "top_features": []}
            
            # Utiliser Random Forest pour l'importance
            if "prime_brute" in numeric_df.columns:
                target = "prime_brute"
                features = [col for col in numeric_df.columns if col != target]
                
                if len(features) > 0:
                    rf = RandomForestRegressor(n_estimators=50, random_state=42)
                    rf.fit(numeric_df[features], numeric_df[target])
                    
                    importance_dict = dict(zip(features, rf.feature_importances_))
                    top_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)[:10]
                    
                    return {
                        "importance": importance_dict,
                        "top_features": [{"feature": f, "importance": round(i, 3)} for f, i in top_features]
                    }
            
            return {"importance": {}, "top_features": []}
            
        except Exception as e:
            logger.error(f"Erreur analyse importance: {e}")
            return {"importance": {}, "top_features": []}
    
    async def _detect_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """D√©tection de patterns avec IA"""
        patterns = {
            "temporal": {},
            "seasonal": {},
            "correlations": {},
            "anomalies": {}
        }
        
        try:
            # Patterns temporels
            if "date_effet" in df.columns:
                dates = pd.to_datetime(df["date_effet"], errors='coerce')
                if dates.notna().any():
                    patterns["temporal"] = {
                        "date_range": f"{dates.min()} √† {dates.max()}",
                        "years_span": (dates.max() - dates.min()).days / 365.25,
                        "monthly_distribution": dates.dt.month.value_counts().to_dict()
                    }
            
            # Corr√©lations importantes
            numeric_df = df.select_dtypes(include=[np.number])
            if len(numeric_df.columns) > 1:
                corr_matrix = numeric_df.corr()
                high_corr = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) > 0.7:
                            high_corr.append({
                                "var1": corr_matrix.columns[i],
                                "var2": corr_matrix.columns[j],
                                "correlation": round(corr_val, 3)
                            })
                patterns["correlations"] = high_corr[:5]  # Top 5
            
            # D√©tection d'anomalies
            if len(numeric_df.columns) >= 2:
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                anomaly_scores = iso_forest.fit_predict(numeric_df.fillna(0))
                anomaly_count = (anomaly_scores == -1).sum()
                patterns["anomalies"] = {
                    "count": int(anomaly_count),
                    "percentage": round(anomaly_count / len(df) * 100, 2)
                }
        
        except Exception as e:
            logger.error(f"Erreur d√©tection patterns: {e}")
        
        return patterns
    
    def _generate_smart_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """G√©n√©ration de recommandations intelligentes"""
        recommendations = []
        
        # Bas√© sur la qualit√© des donn√©es
        quality = analysis["data_quality"]
        if quality["score"] < 0.7:
            recommendations.append("üîß Am√©liorer la qualit√© des donn√©es avant l'analyse")
        
        if quality["completeness"] < 0.9:
            recommendations.append("üìù Traiter les valeurs manquantes")
        
        if quality["consistency"] < 0.8:
            recommendations.append("üéØ Identifier et traiter les outliers")
        
        # Bas√© sur les features
        if analysis["feature_importance"]["top_features"]:
            top_feature = analysis["feature_importance"]["top_features"][0]
            recommendations.append(f"üìä Focus sur '{top_feature['feature']}' (importance: {top_feature['importance']})")
        
        # Bas√© sur les patterns
        patterns = analysis["patterns"]
        if patterns["anomalies"].get("percentage", 0) > 5:
            recommendations.append("üö® Investiguer les anomalies d√©tect√©es")
        
        if len(patterns["correlations"]) > 0:
            recommendations.append("üîó Explorer les corr√©lations fortes d√©tect√©es")
        
        # Recommandations ML
        recommendations.append("ü§ñ Lancer le clustering pour segmentation")
        recommendations.append("üìà Utiliser les mod√®les pr√©dictifs")
        
        return recommendations
    
    def _generate_ai_suggestions(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Suggestions IA avanc√©es"""
        suggestions = []
        
        # Suggestion de mod√®les bas√©e sur les donn√©es
        if len(df) > 1000:
            suggestions.append({
                "type": "model",
                "title": "üéØ Mod√®le de Clustering recommand√©",
                "description": "Dataset volumineux d√©tect√© - clustering DBSCAN optimal",
                "action": "run_clustering",
                "priority": "high",
                "params": {"algorithm": "dbscan", "min_samples": 50}
            })
        
        # Suggestion bas√©e sur les features importantes
        if analysis["feature_importance"]["top_features"]:
            suggestions.append({
                "type": "analysis",
                "title": "üìä Analyse de rentabilit√©",
                "description": "Features importantes d√©tect√©es pour pr√©diction rentabilit√©",
                "action": "profitability_analysis",
                "priority": "medium"
            })
        
        # Suggestion temporelle
        if analysis["patterns"]["temporal"]:
            suggestions.append({
                "type": "temporal",
                "title": "üìÖ Analyse temporelle",
                "description": "Patterns temporels d√©tect√©s - analyse de saisonnalit√© recommand√©e",
                "action": "temporal_analysis",
                "priority": "low"
            })
        
        return suggestions
    
    async def smart_model_selection(self, df: pd.DataFrame, task_type: str) -> Dict[str, Any]:
        """S√©lection intelligente de mod√®le avec Auto-ML"""
        logger.info(f"üß† S√©lection intelligente de mod√®le pour: {task_type}")
        
        model_recommendation = {
            "recommended_algorithm": None,
            "performance_estimate": 0.0,
            "reasoning": "",
            "alternatives": []
        }
        
        # Analyse du dataset pour recommandation
        data_characteristics = {
            "size": len(df),
            "features": len(df.columns),
            "missing_ratio": df.isnull().sum().sum() / (len(df) * len(df.columns)),
            "numeric_ratio": len(df.select_dtypes(include=[np.number]).columns) / len(df.columns)
        }
        
        # Logique de recommandation intelligente
        if task_type == "clustering":
            if data_characteristics["size"] > 10000:
                model_recommendation["recommended_algorithm"] = "dbscan"
                model_recommendation["reasoning"] = "DBSCAN optimal pour gros datasets avec d√©tection automatique du nombre de clusters"
            else:
                model_recommendation["recommended_algorithm"] = "kmeans"
                model_recommendation["reasoning"] = "K-means efficace pour datasets moyens"
        
        elif task_type == "prediction":
            if data_characteristics["features"] > 20:
                model_recommendation["recommended_algorithm"] = "xgboost"
                model_recommendation["reasoning"] = "XGBoost excellent pour nombreuses features"
            else:
                model_recommendation["recommended_algorithm"] = "random_forest"
                model_recommendation["reasoning"] = "Random Forest robuste et interpr√©table"
        
        # Performance estim√©e bas√©e sur les caract√©ristiques
        base_performance = 0.75
        if data_characteristics["missing_ratio"] < 0.1:
            base_performance += 0.1
        if data_characteristics["numeric_ratio"] > 0.8:
            base_performance += 0.05
        
        model_recommendation["performance_estimate"] = min(base_performance, 0.95)
        
        return model_recommendation
    
    async def generate_ai_insights(self, df: pd.DataFrame, results: Dict[str, Any]) -> Dict[str, Any]:
        """G√©n√®re des insights IA √† partir des r√©sultats"""
        insights = {
            "key_findings": [],
            "business_impact": [],
            "recommendations": [],
            "risk_alerts": [],
            "opportunities": []
        }
        
        # Analyse des r√©sultats pour insights
        if "cluster_labels" in results:
            n_clusters = len(np.unique(results["cluster_labels"]))
            insights["key_findings"].append(f"üìä {n_clusters} segments distincts identifi√©s")
            
            if n_clusters > 5:
                insights["business_impact"].append("üéØ Segmentation fine permet personnalisation marketing")
            
        # Alertes risque bas√©es sur anomalies
        if "anomalous_contracts" in results:
            anomaly_count = len(results["anomalous_contracts"])
            if anomaly_count > len(df) * 0.05:  # Plus de 5%
                insights["risk_alerts"].append(f"‚ö†Ô∏è {anomaly_count} contrats anomaliques d√©tect√©s - r√©vision recommand√©e")
        
        # Opportunit√©s bas√©es sur performance
        if "predictions" in results:
            high_profit_contracts = np.array(results["predictions"]) > np.percentile(results["predictions"], 75)
            if high_profit_contracts.any():
                insights["opportunities"].append("üí∞ Contrats haute rentabilit√© identifi√©s pour expansion")
        
        return insights
    
    @lru_cache(maxsize=16)
    def get_model_explanation(self, model_type: str) -> str:
        """Explications des mod√®les en cache"""
        explanations = {
            "xgboost": "üöÄ XGBoost: Gradient boosting optimis√©, excellent pour pr√©dictions pr√©cises",
            "random_forest": "üå≤ Random Forest: Ensemble d'arbres, robuste et interpr√©table",
            "dbscan": "üîç DBSCAN: Clustering bas√© densit√©, d√©tecte automatiquement outliers",
            "kmeans": "üìä K-Means: Clustering centro√Øde, rapide et efficace"
        }
        return explanations.get(model_type, "Mod√®le ML avanc√©")
    
    def save_ai_state(self, filepath: str):
        """Sauvegarde l'√©tat IA"""
        state = {
            "models": {k: v for k, v in self.models.items() if k != "model_object"},
            "insights": self.ai_insights,
            "config": self.auto_ml_config,
            "timestamp": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ √âtat IA sauvegard√©: {filepath}")
    
    def load_ai_state(self, filepath: str):
        """Charge l'√©tat IA"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                state = json.load(f)
            
            self.ai_insights = state.get("insights", {})
            self.auto_ml_config.update(state.get("config", {}))
            
            logger.info(f"üìÅ √âtat IA charg√©: {filepath}")
        except Exception as e:
            logger.error(f"Erreur chargement √©tat IA: {e}")